
# ğŸ¯ Real-Time Contextual Bandit Recommendation Engine  
*A production-ready reinforcement learning engine for real-time personalized recommendations in rewarded mobile apps.*

---

## ğŸš€ Overview  
This project implements a **real-time Contextual Bandit recommender system (LinUCB)** optimized for rewarded apps.  
It selects the **next best action** to increase engagement, retention, and monetization.

Supported high-value actions:
- ğŸ Show bonus offer  
- â­ Invite friend  
- ğŸ“± Suggest new app to explore  
- ğŸ”— Deep link to store  

The engine uses:
âœ”ï¸ User context  
âœ”ï¸ Online learning  
âœ”ï¸ Exploreâ€“exploit optimization  
âœ”ï¸ FastAPI microservice for real-time inference  

---

## ğŸ§  Key Features  
- **Synthetic userâ€“action dataset generator**  
- **LinUCB contextual bandit implementation**  
- **Training pipeline** (learns action weights + confidence bounds)  
- **FastAPI scoring API**  
- **Persistent model storage** (`npz`)  
- **Industry-level project structure**

---

## ğŸ— Architecture  


A Reinforcement Learning system for real-time personalized recommendations in rewarded mobile apps.

ğŸš€ Overview

This project implements a real-time recommendation engine based on Contextual Bandits (LinUCB) to recommend high-value actions such as:

ğŸ Show bonus offer

â­ Invite friend

ğŸ“± Suggest new app to explore

ğŸ”— Deep link to store

The engine uses user context + exploration/exploitation to pick the best next action in real time, backed by an online-learning model.

This repository includes:

âœ… Data simulation
âœ… Training of a LinUCB contextual bandit
âœ… Action scoring API using FastAPI
âœ… Real-time recommendation endpoint
âœ… Reproducible environment + clean project structure

ğŸ§  Architecture

realtime-reco-bandit-engine/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ generate_bandit_data.py # Generates synthetic interaction data
â”‚ â”œâ”€â”€ train_linucb.py # Trains LinUCB model
â”‚ â”œâ”€â”€ linucb_bandit.py # Bandit algorithm implementation
â”‚
â”œâ”€â”€ api_recommender.py # FastAPI microservice (real-time recommendations)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

User Context â†’ Feature Vector â†’ LinUCB Model â†’ UCB Scores â†’ Selected Action

A high-level view:

src/generate_bandit_data.py     â†’ Creates training data
src/linucb_bandit.py            â†’ LinUCB implementation
src/train_linucb.py             â†’ Trains the bandit & saves model
api_recommender.py              â†’ FastAPI real-time scoring API
data/                           â†’ Saved bandit model (linucb_model.npz)

ğŸ“¦ Installation

1. Clone the repository

git clone git@github.com:juanenciso/Real-Time-Contextual-Bandit-Recommendation-Engine.git
cd Real-Time-Contextual-Bandit-Recommendation-Engine

2. Create and activate the virtual environment

python3 -m venv .venv
source .venv/bin/activate

3. Install dependencies

pip install -r requirements.txt

ğŸ›  Training the LinUCB Bandit

Generate simulated bandit data

python src/generate_bandit_data.py

This produces:

data/bandit_simulated.csv

Train the LinUCB model

python src/train_linucb.py

This saves the trained model:

data/linucb_model.npz

âš¡ Run the API Server

uvicorn api_recommender:app --reload --port 8020

Server runs at:

ğŸ‘‰ http://127.0.0.1:8020

ğŸ” API Endpoints

Health check

GET /health

Response:

{
  "status": "ok",
  "model_loaded": true,
  "n_actions": 4,
  "alpha": 1.0
}

ğŸ¯ Real-Time Recommendation Endpoint

POST /recommend_action

Example request:

curl -X POST "http://127.0.0.1:8020/recommend_action" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "u123",
    "country": "AT",
    "device": "ios",
    "segment": "high",
    "n_sessions": 10,
    "days_since_install": 5,
    "recent_engagement": 12,
    "avg_session_length": 180.0
  }'

Example response:

{
  "user_id": "u123",
  "recommended_action": "show_bonus_offer",
  "ucb_score": 1.0327,
  "scores": {
    "show_bonus_offer": 1.0327,
    "suggest_new_app": 0.9185,
    "invite_friend": 0.9970,
    "deep_link_to_store": 1.0113
  },
  "alpha": 1.0
}

ğŸ§® Model: LinUCB Explained

The LinUCB algorithm balances:

Exploration: testing new actions

Exploitation: choosing best known action

Optimization principle:

UCB = expected_reward + Î± * uncertainty

Where:

expected_reward = Î¸áµ€x

uncertainty = sqrt(xáµ€ Aâ»Â¹ x)

This allows the model to adapt in real time as new users interact with the system.


